# Проект 7-го спринта

### Описание переноса данных
1. Перед расчётом витрин нужно перенести данные из /user/master/data/geo/events в
   /user/sppetrov12/data/events. При этом данные разбиты на партиции сначала по дате и времени события, а
   затем по его типу. 

   - /user/sppetrov12/data/events/date=2022-05-31/event_type=message
   - /user/sppetrov12/data/events/date=2022-05-31/event_type=subsrciption
   - /user/sppetrov12/data/events/date=2022-05-31/event_type=reaction

2. В geo.csv был добавлен столбец timezone, с помощью которого можно будет определить локальное время города,
   в котором произошло событие.

3. Сам файл geo.csv находится в /user/sppetrov12/data/geo.csv.

4. Во время переноса данных происходит привязывание координат события к городу с наименьшим расстонием в километрах. Постоянное выполнение cross join во время построения витрин является крайне неэффективным.

   Если у события есть широта и долгота, то во время переноса к нему привязывается город.

   Если у события нет либо широты, либо долготы, то во вместо города и временно зоны выставляется символ "-".

5. Таким образом, к структуре events добавляются четыре поля:
     - city - город события.
     - timezone - временная зона города события.
     - datetime - дата и время возникновения события. Если event.datetime не определён, 
                  то берётся message_ts (дата и время сообщения)
                
     - user_id - пользователь, совершивший событие (подписка, отправка сообщения, реакция).

6. Для удобства, поле lat перименовано на event_lat, а lon - на event_lon.

7. Задача загрузки событий представлена в файле src/srcipts/load_events_job.py.
  
   Ниже приведены параметры запуска:
     - dt - дата, до которой включительно будет загрузка сообщений.
     - depth - глубина в днях. Т.е, события будут загружаться на отрезке [dt - depth; dt].
     - hdfs_url - URL к hdfs.
     - master - способ подключения к Spark (local[*], yarn).
     - uname - логин пользователя (sppetrov12).
     - events_src_path - путь, из которого брать данные по events.
     - events_dst_path - путь, в которой будут сохранены даные по events после преобразования.
     - city_path - путь к geo.csv на hdfs. 

8. DAG-запуска задачи загрузки событий представлен в файле src/dags/load_events_dag.py.

### Описание файла utils.py
1. input_paths - функция, возвращающая пути с данными. Принимает следующие аргументы:
     - sc - объект типа SparkContext.
     - hdfs_url - URL к hdfs.
     - input_path - путь к данным типа events.
     - dt - дата, до которой включительно будет загрузка сообщений.
     - depth - глубина в днях. Т.е, события будут загружаться на отрезке [dt - depth; dt].
     - event_type - тип события, необязательный параметр. По умолчанию, будут выбираться все события.

   Для ускорения выборки идёт обращение к партициям, но если хотя бы одного пути не будет в hdfs, то

   скрипт обработки завершится с ошибкой. Поэтому при формировании списка путей к данными проверяется наличие того или иного пути в hdfs

2. get_events - функция, читающая данные в формате parquet и возвращающая указанные выражения.

   Принимает следующие аргументы:
     - sc - объект типа SparkContext.
	 - s - объект типа SparkSession.
	 - hdfs_url - URL к hdfs.
	 - events_src_path - путь к данным типа events.
	 - dt - дата, до которой включительно будет загрузка сообщений.
	 - depth - глубина в днях. Т.е, события будут загружаться на отрезке [dt - depth; dt].
	 - expr_list - список выражений в select, требуются не все данные events, а только часть из них.
	 - event_type - тип события, необязательный параметр. По умолчанию, будут выбираться все события.
   
### Описание users_datamart
1. Реализация представлена в файле src/srcipts/users_datamart_job.py.

   Ниже приведены параметры запуска:
     - dt - дата, до которой включительно будет загрузка сообщений.
	 - depth - глубина в днях. Т.е, события будут загружаться на отрезке [dt - depth; dt].
	 - hdfs_url - URL к hdfs.
	 - master - способ подключения к Spark (local[*], yarn).
	 - uname - логин пользователя (sppetrov12).
	 - events_src_path - путь, из которого брать преобразованные данные по events.
	 - users_datamart_base_path - путь, в который будут сохранены рассчитанные данные витрины.

2. Данные витрины хранятся по пути /user/sppetrov12/data/analytics/users_datamart/date=dt/depth=dp

3. act_city - город, в котором пользователь отправил своё последнее сообщение.

   В данном случае, множество событий разбивается на группы по user_id.

   Каждая группа упорядочивается по полю datetime в порядке возрастания.

   Таким образом, для получения города нужно использовать функцию last_value.

   Здесь нужно оставить для каждого пользователя только одну строку.

   В данном случае, это строка с номером, равным количеству строк в группе.

4. Рассмотрим алгоритм определения количества дней, проведённых в городе тем или иным пользователем.

   Пусть существует следующее множество:

   | user_id | city | datetime | timezone
   | -| -| -| -
   | 1 | MSK | 2020-12-31 12:45:00 | Europe/Moscow
   | 1 | MSK | 2021-01-01 12:45:00 | Europe/Moscow
   | 1 | MSK | 2021-01-02 00:23:00 | Europe/Moscow
   | 1 | SPB | 2021-01-05 00:22:00 | Europe/Moscow
   | 1 | SPB | 2021-01-06 00:45:00 | Europe/Moscow
   | 1 | SPB | 2021-02-01 01:00:00 | Europe/Moscow

   Для определения количества проведённых дней в городе необходимо определить:
     -  Дату и время, когда пользователь появился в городе.
     -  Дату и время, когда пользователь появился в другом городе.

   Пользователь появился в городе, если его наименование не совпадает с наименованием города предыдущей строки.

   Пользователь находится в городе в последний раз перед переездом, если наименование города не совпадает с наименованием города следующей строки.

   Для этого вычислим столбцы next_city и prev_city

   В результате получим:

   | user_id | city | datetime | timezone | next_city | prev_city
   | -| -| -| -| -|  -|
   | 1 | MSK | 2020-12-31 12:45:00 | Europe/Moscow | MSK | start
   | 1 | MSK | 2021-01-01 01:40:00 | Europe/Moscow | MSK | MSK
   | 1 | MSK | 2021-01-02 00:23:00 | Europe/Moscow | SPB | MSK
   | 1 | SPB | 2021-01-05 00:22:00 | Europe/Moscow | SPB | MSK
   | 1 | SPB | 2021-01-06 00:45:00 | Europe/Moscow | SPB | SPB
   | 1 | SPB | 2021-02-01 01:00:00 | Europe/Moscow | finish | SPB
   
   Оставим только первые и последние появления пользователя в том или ином городе:

   | user_id | city | datetime | timezone | next_city | prev_city
   | -| -| -| -| -|  -|
   | 1 | MSK | 2020-12-31 12:45:00 | Europe/Moscow | MSK | start
   | 1 | MSK | 2021-01-02 00:23:00 | Europe/Moscow | SPB | MSK
   | 1 | SPB | 2021-01-05 00:22:00 | Europe/Moscow | SPB | MSK
   | 1 | SPB | 2021-02-01 01:00:00 | Europe/Moscow | finish | SPB

   Теперь определим дату и время предыдущей и последующей строки:

   | user_id | city | datetime | timezone | next_city | prev_city | next_datetime | prev_datetime
   | -| -| -| -| -|  -| -|  -|
   | 1 | MSK | 2020-12-31 12:45:00 | Europe/Moscow | MSK | start | 2021-01-02 00:23:00 | 2020-12-31 12:45:00
   | 1 | MSK | 2021-01-02 00:23:00 | Europe/Moscow | SPB | MSK | 2021-01-05 00:22:00 | 2020-12-31 12:45:00
   | 1 | SPB | 2021-01-05 00:22:00 | Europe/Moscow | SPB | MSK | 2021-02-01 01:00:00 | 2021-01-02 00:23:00
   | 1 | SPB | 2021-02-01 01:00:00 | Europe/Moscow | finish | SPB | 2021-02-02 01:00:00 | 2021-01-05 00:22:00


   Оставим только последние упоминания о пользователе в том или ином городе перед переездом. 

   Только теперь можно определить количество проведённых дней в том или ином городе:
   
   | user_id | city | datetime | timezone | next_city | prev_city | next_datetime | prev_datetime | diff_in_days
   | -| -| -| -| -|  -| -|  -|  -|
   | 1 | MSK | 2021-01-02 00:23:00 | Europe/Moscow | SPB | MSK | 2021-01-05 00:22:00 | 2020-12-31 12:45:00 | 5
   | 1 | SPB | 2021-02-01 01:00:00 | Europe/Moscow | finish | SPB | 2021-02-02 01:00:00 | 2021-01-05 00:22:00 | 28

   На основе этой выборки можно определять маршрут и домашний город. 

6. Финальная выборка получается путём соединения данных о маршруте с данными об актуальном и домашнем городе.

   Тип соединения left, поскольку пользователь за выбранном отрезок времени может не отправлять сообщения, поэтому у него может не быть актуального города.

   Также, пользователь мог находиться во всех городах маршрута менее 27 дней, поэтому домашнего города также может и не быть.

7. DAG-запуска задачи загрузки событий представлен в файле src/dags/users_datamart_dag.py.

### Описание zones_datamart
1. Реализация представлена в файле src/srcipts/zones_datamart_job.py.

   Ниже приведены параметры запуска:
     - dt - дата, до которой включительно будет загрузка сообщений.
	 - depth - глубина в днях. Т.е, события будут загружаться на отрезке [dt - depth; dt].
	 - hdfs_url - URL к hdfs.
	 - master - способ подключения к Spark (local[*], yarn).
	 - uname - логин пользователя (sppetrov12).
	 - events_src_path - путь, из которого брать преобразованные данные по events.
	 - zones_datamart_base_path - путь, в который будут сохранены рассчитанные данные витрины.

2. Данные витрины хранятся по пути /user/sppetrov12/data/analytics/zones_datamart/date=dt/depth=dp.

3. Здесь datetime урезается до начала месяца и недели соответственно, в результате, появляются значения для полей month и week.

   Сначала исходное множество группируется по полям month и city.

   А затем исходное множество группируется по полям month, week и city.

   Затем эти множества соединяются с помощью join и формируют конечную выборку.

   Опытным путём было выяснено, что данный вариант работал быстрее применения оконной функции по month и city

   и последующей агрегации по month, week и city.

4. DAG-запуска задачи загрузки событий представлен в файле src/dags/zones_datamart_dag.py.

### Описание recommedations_datamart
1. Реализация представлена в файле src/srcipts/recommendations_datamart_job.py.

   Ниже приведены параметры запуска:
     - dt - дата, до которой включительно будет загрузка сообщений.
     - depth - глубина в днях. Т.е, события будут загружаться на отрезке [dt - depth; dt].
     - topn - количество последних событий по datetime по убыванию для каждого пользователя.
     - hdfs_url - URL к hdfs.
     - master - способ подключения к Spark (local[*], yarn).
     - uname - логин пользователя (sppetrov12).
     - events_src_path - путь, из которого брать преобразованные данные по events.
     - recommendations_datamart_base_path - путь, в который будут сохранены рассчитанные данные витрины.

2. Данные витрины хранятся по пути /user/sppetrov12/data/analytics/recommendations_datamart/date=dt/depth=dp.

3. Из информации о подписках выбираем только уникальные пары столбцов user_id и subscription_channel.

   При этом subscription_channel не должен содержать NULL-значения.

   Затем выборка будет соединена сама с собой по полю subscription_channel.

   В результате получим пары пользователей, подписанных на один канал.


4. Затем нужно выбрать за указанный временной отрезок все события типа message, у которых 

   определены message_from и message_to. Их нужно объединить для получения пар пользователей, которые писали дргу другу.

5. Полученные пары подписок соединяются с множеством сообщений типом anti, поскольку нам нужны пары пользователей, которые друг другу не писали.

6. Для каждого пользователя и каждого типа события определяются последние topn событий по datetime.

   Затем topn сообщений, подписок и реакций объединяются между собой и среди них выбираются topn событий по datetime по убыванию.

   Полученные в пункте 5 пары соединяются с последними событиями, для которых определены широта и долгота.

   Получим координаты событий для пары пользователей, оставляем только те пары, у которых расстояние между ними < 1 км.

   Они и составят финальную выборку.

7. DAG-запуска задачи загрузки событий представлен в файле src/dags/recommendations_datamart_dag.py.
